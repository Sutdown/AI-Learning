import os
import json
from dotenv import load_dotenv
from langchain_community.chat_models import ChatTongyi
from langchain.schema import HumanMessage, SystemMessage
from langgraph.graph import StateGraph, END

# è½½å…¥ç¯å¢ƒå˜é‡
load_dotenv()
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY")

# åˆå§‹åŒ–æ¨¡å‹
llm = ChatTongyi(model="qwen3-max", api_key=DASHSCOPE_API_KEY, temperature=0.3)

# -------------------------
# å®šä¹‰èŠ‚ç‚¹å‡½æ•°ï¼ˆGraph Nodesï¼‰
# -------------------------

def extract_products(state):
    """Step 1: ä»ç”¨æˆ·é—®é¢˜ä¸­æŠ½å–å•†å“ä¸ç±»åˆ«"""
    user_msg = state["user_msg"]

    system_prompt = """
ä½ å°†è·å¾—ä¸€æ®µç”µå•†å®¢æœå¯¹è¯ï¼Œè¯·ä»ä¸­æå–å‡ºâ€œå•†å“åç§°â€å’Œâ€œæ‰€å±ç±»åˆ«â€ã€‚
è¾“å‡ºæ ¼å¼ä¸ºJSONæ•°ç»„ï¼Œä¾‹å¦‚ï¼š
[{"category": "Televisions", "product_name": "CineView 8K TV"}]
"""
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_msg)
    ]

    result = llm.invoke(messages)
    try:
        extracted = json.loads(result.content)
    except Exception:
        extracted = [{"category": "Unknown", "product_name": "Unknown"}]

    state["extracted_products"] = extracted
    return state


def query_product_info(state):
    """Step 2: æ ¹æ®è¯†åˆ«ç»“æœä»æœ¬åœ°æ–‡ä»¶æŸ¥æ‰¾å•†å“ä¿¡æ¯"""
    products_file = "products.json"
    if not os.path.exists(products_file):
        raise FileNotFoundError("products.json æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºã€‚")

    with open(products_file, "r", encoding="utf-8") as f:
        all_products = json.load(f)

    extracted = state["extracted_products"]
    product_info_list = []

    for item in extracted:
        name = item.get("product_name", "")
        found = next((p for p in all_products if p["name"] == name), None)
        if found:
            product_info_list.append(found)

    state["product_info"] = product_info_list
    return state


def generate_answer(state):
    """Step 3: åŸºäºå•†å“ä¿¡æ¯ç”Ÿæˆå®¢æœå›ç­”"""
    user_msg = state["user_msg"]
    product_info = state.get("product_info", [])

    info_str = "\n".join([f"{p['name']}ï¼š{p['description']}" for p in product_info])

    system_prompt = """
ä½ æ˜¯ä¸€åç”µå•†æ™ºèƒ½å®¢æœï¼Œè¯·æ ¹æ®æä¾›çš„å•†å“ä¿¡æ¯å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
è‹¥æ— æ³•å›ç­”ï¼Œè¯·ç¤¼è²Œåœ°æç¤ºç”¨æˆ·ã€‚
"""

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=f"ç”¨æˆ·é—®é¢˜ï¼š{user_msg}\n\nå•†å“ä¿¡æ¯ï¼š\n{info_str}")
    ]

    result = llm.invoke(messages)
    state["final_answer"] = result.content
    return state


# -------------------------
# æ„å»º LangGraph æµç¨‹
# -------------------------

graph = StateGraph()

graph.add_node("extract", extract_products)
graph.add_node("query", query_product_info)
graph.add_node("answer", generate_answer)

graph.set_entry_point("extract")
graph.add_edge("extract", "query")
graph.add_edge("query", "answer")
graph.add_edge("answer", END)

app = graph.compile()

# -------------------------
# æ‰§è¡Œå…¥å£
# -------------------------

if __name__ == "__main__":
    user_input = input("ç”¨æˆ·é—®é¢˜ï¼š")
    result = app.invoke({"user_msg": user_input})
    print("\nğŸ¤– å®¢æœå›ç­”ï¼š")
    print(result["final_answer"])
